# cs231n

## Lecture 3

### Multiclass SVM loss  

![image-20200720162837506](E:/postgraduate/notes-of-a-postgraduate/images/image-20200720162837506.png)

calculate for per class.

![image-20200720162850023](E:/postgraduate/notes-of-a-postgraduate/images/image-20200720162850023.png)

![image-20200720163919186](E:/postgraduate/notes-of-a-postgraduate/images/image-20200720163919186.png)

Compared with $S_{y_{i}}$ , if $S_{j}$ is smaller over the margin (here is 1),  then the loss is 0. S means score for every class. i means classes of label.

### Regularization

![image-20200720164503110](images/image-20200720164503110.png)

Regularization pushes against fitting the data too well so we donâ€™t fit noise in the data. There are some regularization methods.

![image-20200720164745788](images/image-20200720164745788.png)		

### Why regularize?

- Express preferences over weights
- Make the model simple so it works on test data
- Improve optimization by adding curvature  

![image-20200720173556460](images/image-20200720173556460.png)

## Softmax Classifier

- direct error

![image-20200720174620590](images/image-20200720174620590.png)

- cross-entropy

![image-20200720175249264](images/image-20200720175249264.png)